<!doctype html>
<html class="no-js" lang="en">

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="keywords"
		content="blog,personal,responsive,search,font awesome,pages,posts,multilingual,highlight.js,syntax highlighting,premium,shortcuts">
	<title>Submitting, Inspecting and Cancelling PBS Jobs - HPC Wiki</title>
	<meta itemprop="name" content="WIKI">
	<meta property="og:title" content="WIKI">
	<meta property="og:url" content="/page/wiki/pages/">
	<meta property="og:site_name" content="High Performance Computing Center">
	<meta property="og:type" content="article">


	<link href="../" rel="alternate" type="application/rss+xml" title="High Performance Computing Center" />
	<link href="../" rel="feed" type="application/rss+xml" title="High Performance Computing Center" />


	<link rel="shortcut icon" href="../../../logo.ico">

	<link rel="stylesheet" href="../../../theme.css">

	<style>
		.output {
			display: block;
			font-family: monospace, monospace;
			white-space: pre-line;
		}
	</style>
	
</head>

<body class="BilboBaggins-theme">


	<nav>

		<div class="container">
			<ul class="topnav">


				<li><a href="../../about/index.html">About</a></li>



				<li><a href="../../news/index.html">News</a></li>



				<li><a href="../../people/index.html">People</a></li>



				<li><a href="../../wiki/index.html">WIKI</a></li>
				

            
				<li><a href="../../research-support/index.html">Research support</a></li>
				


				<li><a href="../../forms/index.html">Forms</a></li>
            
            
            
				<li><a href="../../privacy/index.html">Privacy policy</a></li>


        		<li><a href="../../contacts/index.html">Contacts</a></li>

			</ul>

		</div>
	</nav>



	<header>

		<div class="container">
			<div class="logo">
				<a id="siteBaseUrl" href="../../../index.html" class="logo">

					<img src="../../../logo.png" alt="" height=177>

				</a>
			</div>
			<div class="titles">
				<h3 class="title"><a href="../../../index.html">High Performance Computing Center</a></h3>

				<span class="subtitle">Scuola Normale Superiore</span>

			</div>

			<div class="toggler">

				<i class="fa fa-bars" aria-hidden="true"></i>
			</div>
		</div>
	</header>


	<div class="main container">

		<div class="article-wrapper u-cf single">

			<article class="default article">


				<div class="content">

					<div id="content" class="mw-body" role="main">

						<h1 id="firstHeading" class="firstHeading" lang="en">
							Submitting, Inspecting and Cancelling PBS Jobs
						</h1>
						<div id="bodyContent" class="mw-body-content">
							<div id="mw-content-text" dir="ltr" class="mw-content-ltr" lang="en">
								<div class="mw-parser-output">

									<br/>
									<h2><span class="mw-headline" id="Brief_introduction_to_PBS">
										About  job scheduling systems
									</span></h2>
									<p>
										A "job" is essentially a set of commands to be executed on 
										the compute nodes, together with a specification of the 
										computational resources (number of nodes, number of processors 
										per node, amount of RAM memory per node, execution time ecc.) 
										to be reserved for their execution.
									</p>
									<p>
										Job requests are submitted to a job scheduling system (e.g. PBS). 
										The main purpose of the job scheduler is to dispatch jobs to 
										compute nodes for execution, trying both to maximize and balance 
										the utilization of the computational resources.
									</p>
									<p>
										Once submitted, the job is appended to a "queue" in which it will 
										be pending until there will be enough free resources to allow its 
										execution. It is common for a cluster to provide different queues, 
										each associated to a different subset of compute nodes, a different 
										maximum execution time etc.
									</p>
									
									<hr>
									<p><b>IMPORTANT NOTE</b></p>
									<p>
										All calculations <u><b>MUST</b></u> be submitted as Jobs to the 
										<b>Portable Batch System (PBS) scheduling system</b>, 
										for their execution on the compute nodes.
									</p>
									<p>
										<b>
											Running interactively on the "head-nodes" is FORBIDDEN.
											It is also STRICTLY FORBIDDEN to run your computations
											on the compute nodes bypassing the job submission mechanism.
										</b>
									</p>
									<hr>

									<h2><span class="mw-headline" id="Brief_introduction_to_PBS">
											Brief introduction to PBS
									</span></h2>
									<p>
										Foreword: the following instructions have been written for BASH; 
										however adapting them to CSH or TCSH is straightforward.
									</p>
									<p>
										Remember that you can always access the online manuals using the command
										<code>man</code>. Also, we strongly suggest to read the PBS user manual,
										you can download it from the 
										<a 	rel="nofollow" class="external text"
											href="https://www.altair.com/pdfs/pbsworks/PBSUserGuide19.2.3.pdf">
											Altair website
										</a>.
									</p>

									<h3><span class="mw-headline" id="What_is_a_PBS_job_and_its_basic_commands">
										What is a PBS job and its basic commands
									</span></h3>
									<p>
										In this section we are going to submit our first <b>PBS job</b>. A
										PBS job is simply a shell script, possibly with some PBS directives. 
										PBS directives look like shell comments, so the shell itself ignores them,
										but PBS picks them up and processes the job accordingly. A BASH scripts
										usually begins with <code>#!/bin/bash</code>, or if you prefer TCSH
										<code>#!/bin/tcsh</code>.
									</p>
									<p>
										We are going to start by submitting a very simple shell script, that
										executes two Unix commands and then exits; it doesn't have any PBS
										directives. The script must be executable:
									</p>
									<p class="output">$ pwd
										/home/hpcstaff/PBS

										$ ls -l
										-rw-r--r--  1 hpcstaff hpcstaff   76 Dec  2 22:50 job.sh

										$ cat job.sh
										#!/bin/bash
										hostname
										date
										exit 0

										$ chmod +x job.sh
									</p>

									<h4><span class="mw-headline" id="qsub">qsub</span></h4>
									<p>
										The job is submitted with the command <code>qsub</code>:
									</p>
									<p class="output">$ qsub -q q07daneel job.sh
										12248.pbs01</p>
									<p>
										The command output is its job ID. It's the same ID that appears 
										in the first column of <code>qstat</code> listing (the qstat 
										command is discussed in the next section). The standard output 
										(STDOUT) of a job is written on a file that, at the end of the 
										job's execution, is copied in the same directory from which the 
										job was submitted. Standard error (STDERR) is also returned on 
										another file in the same directory:
									</p>
									<p class="output">$ ls
										job.sh  job.sh.e12248  job.sh.o12248</p>
									<p>
										The output file name has this format:
									</p>
									<p class="output">job name + .o + job ID number</p>
									<p>
										The same goes for the errors file name, with <i>.e</i> instead 
										of <i>.o</i>. In our example the error file is empty and the 
										standard output file contains:
									</p>
									<p class="output">$ cat job.sh.o12248
										daneel03
										Sun Sep  7 16:27:25 CEST 2015</p>
									<p>
										and this tells us that the job was run on <code>daneel03</code>.<br/>
										Our job executes so fast that we can hardly catch it in action. We are
										going to slow it down by letting it sleep for a hundred seconds before
										exiting. Here is our modified version.
									</p>
									<p class="output">$ cat job.sh
										#!/bin/bash
										hostname
										date
										sleep 100
										date
										exit 0</p>
									<p>
										And just to make sure that it's not going to hang forever, we are
										going to execute it interactively and check that it sleeps for 100
										seconds only:
									</p>
									<p class="output">$ time ./job.sh
										trantor01
										Sun Sep  7 16:53:41 CEST 2015
										Sun Sep  7 16:55:21 CEST 2015
										
										real	1m40.029s
										user	0m0.000s
										sys 	0m0.010s</p>
									<p>
										This worked just fine: the job took 1 minute and 40 seconds, 
										which is 100 seconds, to execute. Now we are going to submit 
										it with <code>qsub</code>:
									</p>
									<p class="output">$ qsub -q q07daneel job.sh
										12259.pbs01</p>
									<p>
										Now, how can you check its state?
									</p>

									<h4><span class="mw-headline" id="qstat">qstat</span></h4>
									<p>
										The <code>qstat</code> command is used to get information 
										about jobs and queues. To inspect the state of a specific job, 
										run <code>qstat</code> with the job ID as argument:
									</p>
									<pre>$ qstat 12259.pbs01
Job id            Name             User              Time Use S Queue
----------------  ---------------- ----------------  -------- - -----
12259.pbs01       job.sh           hpcstaff          00:00:29 R daneel03</pre>
									<p>
										Pay attention to the 5th column, which reports the status of the job. "R" means
										that the job is running. Others common status values are the following:
									</p>
									<dl>
										<dt>E</dt>
										<dd>the job is exiting after having run</dd>
										<dt>H</dt>
										<dd>the job is held - this means that it is not going to run until it is
											released</dd>
										<dt>Q</dt>
										<dd>the job is queued and will run when the requested resources will become
											available</dd>
										<dt>R</dt>
										<dd>the job is running</dd>
										<dt>T</dt>
										<dd>the job is being transferred to a new location - this may happen, e.g., if
											the node the job had been running on crashed</dd>
										<dt>W</dt>
										<dd>the job is waiting to be executed at a later time - you you can specify a
											time after which the job is eligible to run (see section "6.8 Deferring
											Execution"
											of the <a rel="nofollow" class="external text"
												href="https://www.altair.com/pdfs/pbsworks/PBSUserGuide19.2.3.pdf">PBS
												User's Guide</a> for details).
											<br/><br/></dd>
									</dl>
									<p>Other commonly used arguments for the <code>qstat</code> command are the following:
									</p>
									<dl>
										<dt><code>qstat -n1</code>&nbsp;</dt>
										<dd>lists all jobs on system, together with their execution host and running time (walltime).</dd>
										<dt><code>qstat -n1 jobid</code>&nbsp;</dt>
										<dd>displays the state of the specified job, it's running time (walltime) and the execution host.</dd>
										<dt><code>qstat -s jobid</code>&nbsp;</dt>
										<dd>displays the state of the specified job, followed by any comment added by the administrator or scheduler</dd>
										<dt><code>qstat -f jobid</code>&nbsp;</dt>
										<dd>displays full information about the status of a job</dd>
										<dt><code>qstat -fx jobid</code>&nbsp;</dt>
										<dd>displays full information about a finished job</dd>
										<dt><code>qstat -u username</code>&nbsp;</dt>
										<dd>lists all jobs owned by the specified user </dd>
										<dt><code>qstat -q</code>&nbsp;</dt>
										<dd>lists the available queues, with details about the maximum resources that
											can be requested by jobs submitted in each queue</dd>
										<dt><code>qstat -Q</code>&nbsp;</dt>
										<dd>lists the available queues, with details about their status (whether the
											queue is enabled, number of running jobs, number of queued jobs etc.).</dd>
										<dt><code>qstat -Qf queuename </code>&nbsp;</dt>
										<dd>displays full information about the status of a queue</dd>
										<dt><code>qstat -B </code>&nbsp;</dt>
										<dd>lists summary information about the PBS server</dd>
									</dl>

									<h4><span class="mw-headline" id="qdel">qdel</span></h4>
									<p>
										To delete a job use <code>qdel</code>. If the job is running, 
										the command sends <code>SIGKILL</code> to it. If the job is 
										merely queued, the command removes it from the queue. 
										Here's an example:
									</p>
									<p class="output">$ qsub -q q07daneel job.sh
										12390.pbs01
										
										$ qdel 12390.pbs01
										
										$ ls
										job.sh  job.sh.e12390  job.sh.o12390
										
										$ cat job.sh.o12390
										daneel02
										Sun Sep  7 17:26:01 CEST 2015</p>
									
									<h4><span class="mw-headline" id="pbsnodes">pbsnodes</span></h4>
									<p>
										To check the availability of a node that you want 
										to request for your submission, you can use:<br/>
									</p>
									<dl>
										<dt><code>pbsnodes -a</code>&nbsp;</dt>
										<dd>lists all nodes with their features and current state 
											(free, job-busy, stale, etc.). It's quite verbose.</dd>
										<dt><code>pbsnodes &lt;nodeid&gt;</code>&nbsp;</dt>
										<dd>displays features and state of the specified node.</dd>
										<dt><code>pbsnodes -l</code>&nbsp;</dt>
										<dd>lists all DOWN and OFFLINE nodes (which you can't request immediately)</dd>
									</dl>

									<h2><span class="mw-headline" id="Resources_and_queues">
										Resources and queues
									</span></h2>
									<p>
										When you submit a job, you ask for a certain amount of
										resources  to be reserved for that job's execution. 
										There are two types of resources that can be requested:
										"chunk" resources and "job-wide" resources.
									</p>
									<p>
										A "chunk" is a collection of host-level resources 
										(such as a given number of CPUs, a given amount of memory etc.) 
										that are reserved as a unit. The "chunk" is used by the portion 
										of the job which runs on the host on which the resources have 
										been allocated. These resources are requested inside the
										<code>select</code> statement. 
									</p>
									<p>
										"Job-wide" are resources that apply to the entire job, 
										such as the cpu-time or the walltime. These resources 
										are requested outside the <code>select</code> statement. 
										You can request for resources by means of dedicated directives 
										within the job script, or by exploiting the <code>-l</code> 
										option of the qsub command.
									</p>
									<p>
										As an example, consider the following job script:
									</p>
									<p class="output">#!/bin/bash
										#PBS -l select=1:ncpus=2:ngpus=1:mem=8096mb
										#PBS -l walltime=03:00:00
										#PBS -q q07daneel
										#PBS -N sleep
										sleep 3h</p>
									<p>
										In this case, the job needs 1 "vnode" (i.e. compute node), 2 CPUs,
										1 GPU and 8GB of RAM to run. Also, the user is requesting that the 
										job is queued to a specific queue (q07daneel in the example) with 
										the <code>-q</code> option. Please note that on our cluster the 
										queue name <b>must</b> be selected since there is no default queue. 
										<code>-N</code> gives a name to the job.
									</p>
									<p>
										Of course, you can request resources directly within the qsub command:
									</p>
									<p class="output">$ qsub -l select=1:ncpus=2:ngpus=1:mem=8gb -q q07daneel [...] job.sh</p>
									<p>
										The following list describes the resources that can be requested 
										for a job and their default values:
										<ul style="margin-left: 3rem;">
											<li>
												<b><i>ncpus</i></b> ["chunk" resource] -
												The number of logical CPU cores to be reserved. Default value: "1".
											</li>
											<li>
												<b><i>mem</i></b> ["chunk" resource] -
												The amount of RAM memory to be reserved. Default value: "2048mb".
											</li>
											<li>
												<b><i>ngpus</i></b> ["chunk" resource] -
												The number of GPUs to be reserved. Default value: "0".<br/>
												<i>	Note: this resource is available only on q07daneel 
													and q14daneel queues (see below).</i> 
											</li>
											<li>
												<b><i>mpiprocs</i></b> ["chunk" resource] -
												Number of MPI processes for the chunk.
											</li>
											<li>
												<b><i>host</i></b> ["chunk" resource] -
												The compute node on which the job must be executed.<br/>
												<i>	Note: please avoid forcing the execution of your job on 
													a specific host unless you have very good reasons for doing so!</i>
											</li>
											<li>
												<b><i>walltime</i></b> ["job-wide" resource] -
												The maximum execution time for the Job. It's default value 
												depends on the selected queue. If the Job is still executing 
												when the requested walltime expires, it will be killed.<br/>
												<i>	Note: Usually there is no need to explicitly request this resource. 
													Just rely on the queue's limit.</i>
											</li>
										</ul>
									</p>
									<p>
										The resources that have been requested for a job are reserved 
										at the Linux Kernel level when the Job starts its execution, 
										and are not available to other users. Likewise, your job can 
										only use the resources you requested for it. In particular:
										<ul style="margin-left: 3rem;">
											<li>
												Even if your Job spawns more processes / threads than 
												the number of CPU cores that have been assigned to it, 
												all these processes / threads will be executed only by 
												the assigned CPU cores.<br/>
												<i>
													Note: from your code/script you can retrieve the 
													number of cores assigned to the job by reading the 
													"<b>NCPUS</b>" environment variable.
												</i>
											</li>
											<li>
												On Daneel nodes, you'll be able to use only the GPUs 
												you requested for the job. Note that while your program 
												may see all the installed GPUs, the ones you don't 
												reserved will be detected as "incompatible" (or something 
												similar). When possible, avoid passing to your program/library 
												the IDs of the GPUs to be used. Instead, let the program/library 
												detect the available GPUs by itself.
											</li>
											<li>
												As regards to RAM memory, a "soft" limit is enforced: 
												as long as there is enough free memory on the system, 
												the job is allowed to allocate more memory than the reserved amount. 
												However, in the case of shortage of memory, the memory pages of the 
												processes that violate the limit will be swapped out to disk. 
												This usually leads to a sensible slowdown of your computation and, 
												if also the swap area on disk gets filled, to the kill of your job.
											</li>
										</ul>
									</p>
									<p>
										It is also important to note that, on each node, ~4GB of RAM are reserved for
										the Operating System. As a consequence, <b>the maximum amount of memory you
										can request for a job is about 4.5 GB less than the total amount of RAM
										installed on the node</b>. As an example, you should not request more than
										15.5 GB of memory for a job intended to be run on an Helicon node, since
										these nodes are equipped with 20GB of RAM.
									</p>
									<div>
										<p>
											The available queues are listed below. Queues are characterized 
											by the hosts the jobs will be scheduled to and by the maximum
											execution time ("walltime").
											Finally, note that some queues do not allow to start 
											<a href="#faq_interactive_session">interactive job sessions</a>.
										</p>
										
										<ul style="margin-left: 3rem;">
											<li>
												<b><i>q02daneel</i></b><br/>
												Execution hosts: "Daneel" nodes<br/>
												Max walltime: 2 days<br/>
												Max 6 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q07daneel</i></b><br/>
												Execution hosts: "Daneel" nodes<br/>
												Max walltime: 7 days<br/>
												Max 2 running jobs and 10 queued jobs per user<br/>
												Interactive sessions: <em>denied</em>
											</li>
											<li>
												<b><i>q14daneel</i></b><br/>
												Execution hosts: "Daneel" nodes<br/>
												Max walltime: 14 days<br/>
												Max 2 running jobs and 10 queued jobs per user<br/>
												Interactive sessions: <em>denied</em>
											</li>
											<li>
												<b><i>q02hal</i></b><br/>
												Execution hosts: "Hal" nodes<br/>
												Max walltime: 2 days<br/>
												Max 6 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q07hal</i></b><br/>
												Execution hosts: "Hal" nodes<br/>
												Max walltime: 7 days<br/>
												Max 1 running jobs and 5 queued jobs per user<br/>
												Interactive sessions: <em>denied</em>
											</li>
											<li>
												<b><i>q14hal</i></b><br/>
												Execution hosts: "Hal" nodes<br/>
												Max walltime: 14 days<br/>
												Max 1 running jobs and 5 queued jobs per user<br/>
												Interactive sessions: <em>denied</em>
											</li>
											<li>
												<b><i>q02helicon</i></b><br/>
												Execution hosts: "Helicon" nodes<br/>
												Max walltime: 2 days<br/>
												Max 6 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q07helicon</i></b><br/>
												Execution hosts: "Helicon" nodes<br/>
												Max walltime: 7 days<br/>
												Max 4 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>denied</em>
											</li>
											<li>
												<b><i>q14helicon</i></b><br/>
												Execution hosts: "Helicon" nodes<br/>
												Max walltime: 14 days<br/>
												Max 4 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>denied</em>
											</li>
											<li>
												<b><i>q07artes</i></b><br/>
												<i>[Restricted to the members of the "artes" group]</i><br/>
												Execution hosts: "Artes" nodes<br/>
												Max walltime: 7 days<br/>
												Max 3 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q14artes</i></b><br/>
												<i>[Restricted to the members of the "artes" group]</i><br/>
												Execution hosts: "Artes" nodes<br/>
												Max walltime: 14 days<br/>
												Max 3 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q07diamond</i></b><br/>
												<i>[Restricted to the members of the "diamond" group]</i><br/>
												Execution hosts: "Diamond" nodes<br/>
												Max walltime: 7 days<br/>
												Max 10 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q14diamond</i></b><br/>
												<i>[Restricted to the members of the "diamond" group]</i><br/>
												Execution hosts: "Diamond" nodes<br/>
												Max walltime: 14 days<br/>
												Max 7 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q07aurora</i></b><br/>
												<i>[Restricted to the members of the "diamond" group]</i><br/>
												Execution hosts: "Aurora" nodes<br/>
												Max walltime: 7 days<br/>
												Max 3 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q14aurora</i></b><br/>
												<i>[Restricted to the members of the "diamond" group]</i><br/>
												Execution hosts: "Aurora" nodes<br/>
												Max walltime: 14 days<br/>
												Max 3 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q07kalgan</i></b><br/>
												<i>[Restricted to the members of the "kalgan" group]</i><br/>
												Execution hosts: "Kalgan" nodes<br/>
												Max walltime: 7 days<br/>
												Max 3 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q14kalgan</i></b><br/>
												<i>[Restricted to the members of the "kalgan" group]</i><br/>
												Execution hosts: "Kalgan" nodes<br/>
												Max walltime: 14 days<br/>
												Max 3 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q05hypnos</i></b><br/>
												<i>[Restricted to the members of the "interstellar" group]</i><br/>
												Execution hosts: "Hypnos" nodes<br/>
												Max walltime: 5 days<br/>
												Max 3 running jobs per user and 6 queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q05oromasdes</i></b><br/>
												<i>[Restricted to the members of the "astro" group]</i><br/>
												Execution hosts: oromasdes03 (previously called Ananke)<br/>
												Max walltime: 5 days<br/>
												Max 3 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q07cinna</i></b><br/>
												<i>[Restricted to the members of the "compnanobio" group]</i><br/>
												Execution hosts: "Cinna" nodes<br/>
												Max walltime: 7 days<br/>
												Max 3 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q14cinna</i></b><br/>
												<i>[Restricted to the members of the "compnanobio" group]</i><br/>
												Execution hosts: "Cinna" nodes<br/>
												Max walltime: 14 days<br/>
												Max 3 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q02anacreon</i></b><br/>
												<i>[Restricted to the members of the "bioinfo" group]</i><br/>
												Execution hosts: "Anacreon" nodes<br/>
												Max walltime: 2 days<br/>
												No limits on per-user running and queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q07anacreon</i></b><br/>
												<i>[Restricted to the members of the "bioinfo" group]</i><br/>
												Execution hosts: "Anacreon" nodes<br/>
												Max walltime: 7 days<br/>
												Max 2 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>denied</em>
											</li>
											<li>
												<b><i>q14anacreon</i></b><br/>
												<i>[Restricted to the members of the "bioinfo" group]</i><br/>
												Execution hosts: "Anacreon" nodes<br/>
												Max walltime: 14 days<br/>
												Max 2 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>denied</em>
											</li>
											<li>
												<b><i>q02gaia</i></b><br/>
												<i>[Restricted to the members of the "bioinfo" group]</i><br/>
												Execution hosts: "Gaia" nodes<br/>
												Max walltime: 2 days<br/>
												No limits on per-user running and queued jobs<br/>
												Interactive sessions: <em>allowed</em>
											</li>
											<li>
												<b><i>q07gaia</i></b><br/>
												<i>[Restricted to the members of the "bioinfo" group]</i><br/>
												Execution hosts: "Gaia" nodes<br/>
												Max walltime: 7 days<br/>
												Max 2 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>denied</em>
											</li>
											<li>
												<b><i>q14gaia</i></b><br/>
												<i>[Restricted to the members of the "bioinfo" group]</i><br/>
												Execution hosts: "Gaia" nodes<br/>
												Max walltime: 14 days<br/>
												Max 2 running jobs per user. No limits on queued jobs<br/>
												Interactive sessions: <em>denied</em>
											</li>
										</ul>
									</div>	
									<p>
										<u><i>
										Please try to select nodes and queues according to the real necessities
										of your calculation, and avoid crowding on the newest nodes only.<br/>
										Also, remember to specify a queue, since there is no default queue on our cluster.
										</i></u>
									</p>
									<p>
										For further information, please consult the official
										<a 	rel="nofollow" style="color: green;"
											href="https://www.altair.com/pdfs/pbsworks/PBSUserGuide19.2.3.pdf">
											PBS Pro user guide
										</a>.
									</p>

									<h2><span class="mw-headline" id="Scratch_Areas">
										Scratch Areas
									</span></h2>
									<p>
										Every user has a scratch space on every computing node, under <code>/scratch/$USER</code>.
										This area is a temporary storage designed for Jobs' I/O operations.
									</p>
									<p>
										When possible, this storage area is allocated on the 
										<i>local hard drives</i> of the compute nodes, 
										thus providing a <i>higher bandwidth and a lower latency</i>
										than NFS mount points. This is the case, for example,
										of Daneel and Hal nodes. Helicon and Artes, instead, 
										are only equipped with a "<i>shared scratch area</i>": 
										this is a NFS storage space which is accessible by all
										the Helicon and Artes nodes.
									</p>
									<p>
										<b>A few important notes on the use of scratch areas</b>:
										<ul style="margin-left: 3rem;">
											<li>
												<u>You should perform I/O operations on this area and, 
												when the computation is over, copy any relevant output 
												file back to your home directory or 
												<a href="Resources_available_on_Trantor.html#project_areas">
												project folder</a>.</u>
											</li>
											<li>
												<u>At the end of your computation, remember to copy any relevant 
												file stored in the scratch folder back into your home or 
												<a href="Resources_available_on_Trantor.html#project_areas">
												project folder</a>.</u>
												In fact, <b>local scratch areas are not backed up and can be erased 
												by the technical staff</b> for maintenance purposes.
											</li>
											<li>
												<u>Remember to clean up your scratch folder by deleting the files
												you don't need anymore (especially if you killed your job with qdel).</u>
												Note that some computational software creates very big temporary files,
												so it is mandatory that you clean your scratch area from unnecessary 
												files at least every month. The staff will delete them anyway, 
												when necessary, without further notice.
											</li>
											<li>
												<u>Using your home directory or a project folder as a scratch space is strictly forbidden!</u> 
											</li>
										</ul>
									</p>
									<p>
										Here's a recap of the scratch areas available in each nodes group:
										<ul style="margin-left: 3rem;">
											<li>
												<b><i>Daneel</i></b> : 6TB local scratch.
											</li>
											<li>
												<b><i>Hal</i></b> : 11TB local scratch.
											</li>
											<li>
												<b><i>Diamond</i></b> : 3TB local scratch.
											</li>
											<li>
												<b><i>Aurora</i></b> : 4.9TB local scratch.
											</li>
											<li>
												<b><i>Kalgan</i></b> : 8.7TB local scratch.
											</li>
											<li>
												<b><i>Artes and Helicons</i></b> : 25TB per-user <b>shared</b> scratch.
											</li>
											<li>
												<b><i>Oromasdes03 (previously called Ananke)</i></b> : 1.1TB local scratch.
											</li>
											<li>
												<b><i>Hypnos</i></b> : 1.7TB local scratch (mounted under <em>/scratch</em>)
												plus an additional 9.8TB <b>shared</b> scratch area with SSD cache 
												(mounted under <em>/scratch_ssd</em>).
											</li>
											<li>
												<b><i>Cinna</i></b> : small local scratch area.
											</li>
											<li>
												<b><i>Anacreon</i></b> : small local scratch area.
											</li>
											<li>
												<b><i>Gaia01</i></b> : 890GB local scratch.
											</li>
										</ul>
									</p>
									<p>
										You can use the following commands on the computing node 
										to check how much space you are currently using:
										<p class="output">$ cd /scratch/$USER
											$ pwd
											/scratch/yourusername
											$ du -hs (this command could take a while)
											7.5G</p>
									</p>
									
									<!--
									<h2><span class="mw-headline" id="Using_projects">
										Using projects
									</span></h2>
									<hr>
									<p>
										<b>Note:</b> At this time there is no active project. Consequently,
										the <code>mybalance</code> command is not currently available.
									</p>
									<hr>

									<p>
										Some of the queues on the cluster can be used only if you are part of
										the related PBS project and the project has a sufficient budget (number
										of hours) available at the time of submission.
									</p>
									<p>
										To use them, first check with the command <code>mybalance</code> which projects
										you are part of and how many hours are available. For example:
									</p>
									<pre>[hpcstaff@trantor01]$ mybalance

Balance for user hpcstaff

Project           Available hours
------------------------------
account_test      1000000h 0m 0s (1000000.0 hrs)</pre>
									<p>
										To submit your job, use the flag <code>-P projectname</code>. 
										In our example:
									</p>
									<p class="output">qsub -q q07daneel -l select=1:ncpus=2 -P account_test myscript.sh</p>
									<p>Before queueing the job, PBS will check if your project have enough
										hours for the job and will assume that you will use the whole length of
										the queue (in the previous example 7 days * 2 CPUs <br /> = 168 hours * 2 CPUs
										= 336 hours), then
										the hours will be deducted from your budget the moment the job starts.
										If your job will run for less time, the difference will be added back to
										the project budget.<br>
										If the project you are using doesn't have enough hours for your job you will
										receive the error message
										<code>The project 'yourproject' doesn't have the budget for this job</code>.
									</p>
									-->

									<h2><span class="mw-headline" id="FAQ">FAQ</span></h2>

									<h3 id="faq_interactive_session"><span class="mw-headline">
										Requesting an interactive session, the correct way to directly access a node
									</span></h3>
									<p>
										If you need to directly access a computation node, for example to test your job,
										you can use the flag <code>-I</code> (that is a capital i) of qsub to request an
										interactive session. For example:
									</p>
									<p class="output">$ qsub -I -q q07daneel -l select=1:ncpus=4</p>
									<div>
										<p>
											<strong>IMPORTANT NOTE:</strong><br/>
											Interactive sessions are forbidden on the 7 and 14
											days long queues for the following node groups:
											<ul style="margin-left: 3rem;">
												<li><em>daneel</em></li>
												<li><em>hal</em></li>
												<li><em>helicon</em></li>
												<li><em>anacreon</em></li>
												<li><em>gaia</em></li>
											</ul>
										</p>
										
										<p>
											If you want to start an interactive session on these nodes,
											<strong>use the 2 days long queues</strong> instead.
										</p>
									</div>
									
									
									<h3><span class="mw-headline">
										I made a bad evaluation of my job resources</span></h3>
									<p>
										Job resources (the ones you previously asked with qsub, ex.
										<code>-l walltime=xx:xx:xx</code>) can be fixed by using the 
										<code>qalter</code> command. For example:
									</p>
									<p class="output">$ qalter -l walltime=xx:xx:xx jobid</p>
									<p>
										You can use <code>qalter</code> command just to <b>decrement</b> 
										the usage of a job resource. If you need to <b>increment a resource</b>, 
										email us at hpcstaff@sns.it and explain your reasons. 
										Then, the staff will proceed with the increase.
									</p>

									<h3><span class="mw-headline">
										What if my execution machine has gone off-line?
									</span></h3>
									<p>
										If the PBS server shuts down, or in the case of network issues 
										between the server and the compute nodes, the jobs will continue 
										their execution. Of course, in the event of a malfunction of a 
										compute node, all the Jobs assigned to that node will be terminated.
										It is important to note that, in this last case, the Jobs on the dead 
										node may still appear as "<b>Running</b>". If you want to check if your job is 
										<b>effectively</b> running, proceed as follows:
									</p>
									<p class="output">$ qstat -u $USER</p>
									<p>
										For each job id you want to check, type:
									</p>
									<p class="output">$ qstat -f &lt;job ID&gt; | grep exec_host</p>
									<p>
										This will print the slave machine on which the job is <b>presumably</b> running.
										So, log into that server and check if there is actually some processes of yours:
									</p>
									<p class="output">$ top -u $USER</p>
									<p>
										If your job is not effectively running, you should provide 
										the deletion of your own jobs with the following command:
									</p>
									<p class="output">$ qdel -W force &lt;job ID&gt;</p>

									<p>
										<b>Final note:</b> PBS can be configured so to automatically 
										re-queued the Jobs affected by a malfunctioning event. 
										However, this behaviour may lead to several other (tricky) 
										problems so it has been disabled 
										(the "-r n" flags are automatically added to every qsub).
									</p>
								</div>

							</div>
						</div>
					</div>
				</div>

				<hr>

				<div class="credits">
					<div class="container">

						<div class="author">
							<a href="" target="_blank">
								2020 HPC Center SNS - Inspired at github.com/Lednerb Bilberry Hugo Theme
							</a>
						</div>
					</div>
				</div>


				<script type="text/javascript" src="../../../theme.js"></script>

</body>

</html>